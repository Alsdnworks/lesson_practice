{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0H-eWiRPhOTL"
      },
      "source": [
        "# 시계열 딥러닝 모델을 사용한 충주댐 유입량 예측\n",
        "\n",
        "## OBJECTIVE\n",
        "- 대상지역 : 충주댐 상류\n",
        "- 평가지표는 MAE, RMSE, R2 score를 사용\n",
        "- 사용데이터는 직접 입력\n",
        "- 최종결과물은 일단위 예측결과 csv 파일과 절차, 코드, 결과를 설명하는 ipynb 파일로 제출\n",
        "## DATA\n",
        "- 충주댐 상류의 유입량 데이터\n",
        "  \n",
        "- 충주댐 상류의 기상 데이터"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEhY8pEthOTP"
      },
      "source": [
        "1. 필요한 도구 호출, 도구 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qn1Nkqe6hOTP",
        "outputId": "559cd346-05df-441b-e005-a9a9bb49d994"
      },
      "outputs": [
        {
          "ename": "OSError",
          "evalue": "[WinError 126] 지정된 모듈을 찾을 수 없습니다. Error loading \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\lib\\shm.dll\" or one of its dependencies.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[72], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MinMaxScaler\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\__init__.py:141\u001b[0m\n\u001b[0;32m    139\u001b[0m                 err \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mWinError(ctypes\u001b[38;5;241m.\u001b[39mget_last_error())\n\u001b[0;32m    140\u001b[0m                 err\u001b[38;5;241m.\u001b[39mstrerror \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Error loading \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdll\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or one of its dependencies.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 141\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    143\u001b[0m     kernel32\u001b[38;5;241m.\u001b[39mSetErrorMode(prev_error_mode)\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_preload_cuda_deps\u001b[39m(lib_folder, lib_name):\n",
            "\u001b[1;31mOSError\u001b[0m: [WinError 126] 지정된 모듈을 찾을 수 없습니다. Error loading \"C:\\Users\\user\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\lib\\shm.dll\" or one of its dependencies."
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.font_manager as fm\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "plt.rc('font', family='NanumBarunGothic')\n",
        "pd.set_option('display.max_columns', 100)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "warnings.filterwarnings('ignore')\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#device = torch.device(\"cpu\") # cpu 사용시"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FxGji8ChOTP"
      },
      "source": [
        "2. 데이터 불러오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cf-cgKgWhOTQ"
      },
      "outputs": [],
      "source": [
        "df_asos=pd.read_parquet('https://github.com/Alsdnworks/lesson_practice/raw/main/ASOS_127.parquet') # 기상 데이터\n",
        "rawdata = pd.read_parquet('https://github.com/Alsdnworks/lesson_practice/raw/main/ChungjuDam_info.parquet') # 유입량 데이터"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L70SPAAdhOTR"
      },
      "source": [
        "3. 데이터 전처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmRG9ejChOTR"
      },
      "outputs": [],
      "source": [
        "target_column = 'Inflow' #유입량 데이터의 컬럼명\n",
        "\n",
        "# 유입량 데이터 전처리\n",
        "ym =pd.DataFrame()\n",
        "ym['일시']=pd.date_range(start='1985-05-02', end='2024-01-01', freq='D')# 기간을 일별로 맞추기 위해 시작일부터 종료일까지의 날짜를 생성 (예: '2016-01-01' '2021-01-01')\n",
        "rawdata['일시'] = pd.to_datetime(rawdata['일시'])\n",
        "rawdata.columns = ['일시', target_column]\n",
        "rawdata.fillna(method='bfill',inplace=True)\n",
        "\n",
        "# ASOS 데이터 전처리\n",
        "df_asos['일시'] = pd.to_datetime(df_asos['일시'])\n",
        "#df_asos['DayRain_1']=df_asos['일강수량(mm)'].shift(1) # 1일 전 강수량 적용, 필요시 주석 해제\n",
        "#df_asos['DayRain_2']=df_asos['일강수량(mm)'].shift(2) # 2일 전 강수량 적용, 필요시 주석 해제\n",
        "df_asos.sort_values(by='일시',inplace=True)\n",
        "df_asos.drop(['지점','지점명','기사'],axis=1,inplace=True)\n",
        "for c in df_asos.columns:\n",
        "    if '시각' in c:\n",
        "        df_asos.drop(c,axis=1,inplace=True)\n",
        "    if '강수' in c:\n",
        "        df_asos.fillna(0,inplace=True)\n",
        "df_asos.fillna(method='bfill',inplace=True)\n",
        "\n",
        "# 유입량과 ASOS 데이터를 합침\n",
        "dataset=pd.merge(df_asos,rawdata,on='일시',how='left')\n",
        "dataset=pd.merge(ym,dataset,on='일시',how='left')\n",
        "dataset.index = dataset['일시']\n",
        "dataset.drop('일시',axis=1,inplace=True)\n",
        "if dataset.isna().sum().sum() != 0:\n",
        "    print('데이터에 결측치가 있습니다.')\n",
        "    raise ValueError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 644
        },
        "id": "vbNa4ph6hOTR",
        "outputId": "9918c444-aa0c-4f56-bada-a76d0502fb33"
      },
      "outputs": [],
      "source": [
        "# Pearson 상관계수를 통해 유입량과의 상관관계가 높은 변수만 선택\n",
        "corr_with_target = dataset.corr()[target_column]\n",
        "#filtered_columns = dataset.columns[abs(corr_with_target) >= 0.35]# 상관계수의 절대값이 0.35보다 큰 변수만 선택하는 기능, 필요시 주석 해제\n",
        "#dataset=dataset[filtered_columns]\n",
        "\n",
        "# 선택된 변수들의 상관계수를 시각화\n",
        "dataset_4EDA = dataset.copy()\n",
        "dataset_4EDA.columns=['10분 최다 강수량', '1시간 최다강수량', '일강수량1', '9-9강수', '일강수량1_lag1','일강수량1_lag2', '일강수량2', '유입량', '일강수량2_lag1', '일강수량2_lag2']\n",
        "dataset_4EDA=dataset_4EDA[['유입량','10분 최다 강수량', '1시간 최다강수량', '9-9강수', '일강수량1', '일강수량1_lag1','일강수량1_lag2', '일강수량2',  '일강수량2_lag1', '일강수량2_lag2']]\n",
        "plt.figure(figsize=(20, 10))\n",
        "sns.heatmap(dataset_4EDA.corr(), annot=True, fmt='.2f', linewidths=2)\n",
        "del dataset_4EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tknBd-TZhOTS"
      },
      "source": [
        "4. 딥러닝 모델 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "###########################################################################\n",
        "TS=float # 딥러닝 학습용데이터와 테스트 데이터 비율 70:30일경우 ex:0.3\n",
        "learning_rate = float # 학습률 (작을수록 학습을 세밀하게 조정) ex:0.001\n",
        "batch_size = int # 배치 사이즈(한번에 학습할 데이터 양) ex:32\n",
        "epochs = int # 학습 횟수 ex:1000\n",
        "hidden_size1 = int # 첫번째 은닉층 노드 수 ex:64\n",
        "hidden_size2 = int # 두번째 은닉층 노드 수 ex:32\n",
        "###########################################################################\n",
        "# 학습 데이터와 테스트 데이터로 분할\n",
        "X = dataset.drop(columns=[target_column])\n",
        "y = dataset[[target_column]].copy()\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=TS, shuffle=False)\n",
        "input_size = x_train.shape[1]\n",
        "output_size = y_train.shape[1]\n",
        "\n",
        "# 데이터 정규화 (정규화란?: https://m.blog.naver.com/PostView.nhn?blogId=youji4ever&logNo=221384104383&proxyReferer=https:%2F%2Fwww.google.com%2F)\n",
        "scaler = MinMaxScaler()\n",
        "y_scaler = MinMaxScaler()\n",
        "scaler.fit(x_train)\n",
        "y_scaler.fit(y_train)\n",
        "\n",
        "# 데이터를 머신러닝 모델에 입력하기 위해 텐서로 변환 (Tensor란?: https://rekt77.tistory.com/102)\n",
        "train_dataset = TensorDataset(torch.FloatTensor(scaler.transform(x_train)), torch.FloatTensor(y_scaler.transform(y_train)))\n",
        "val_loader = DataLoader(TensorDataset(torch.FloatTensor(scaler.transform(x_test)), torch.FloatTensor(y_scaler.transform(y_test))),\\\n",
        "    batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5PxwdG_hOTT"
      },
      "outputs": [],
      "source": [
        "# LSTM 모델 생성\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.hidden_size1 = hidden_size1\n",
        "        self.hidden_size2 = hidden_size2\n",
        "        self.lstm1 = nn.LSTM(input_size, hidden_size1, batch_first=True)\n",
        "        self.lstm2 = nn.LSTM(hidden_size1, hidden_size2, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_size2, output_size)\n",
        "        self.relu = nn.ReLU()\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)\n",
        "        x, _ = self.lstm1(x)\n",
        "        x, _ = self.lstm2(x)\n",
        "        x = self.linear(x[:, -1, :])\n",
        "        return x\n",
        "\n",
        "# BiLSTM 모델 생성\n",
        "class BILSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
        "        super(BILSTM, self).__init__()\n",
        "        self.hidden_size1 = hidden_size1\n",
        "        self.hidden_size2 = hidden_size2\n",
        "        self.lstm1 = nn.LSTM(input_size, hidden_size1, batch_first=True, bidirectional=True)\n",
        "        self.lstm2 = nn.LSTM(hidden_size1*2, hidden_size2, batch_first=True, bidirectional=True)\n",
        "        self.linear = nn.Linear(hidden_size2*2, output_size)\n",
        "        self.relu = nn.ReLU()\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)\n",
        "        x, _ = self.lstm1(x)\n",
        "        x, _ = self.lstm2(x)\n",
        "        x = self.linear(x[:, -1, :])\n",
        "        return x\n",
        "\n",
        "# GRU 모델 생성\n",
        "class GRU(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
        "        super(GRU, self).__init__()\n",
        "        self.hidden_size1 = hidden_size1\n",
        "        self.hidden_size2 = hidden_size2\n",
        "        self.gru1 = nn.GRU(input_size, hidden_size1, batch_first=True)\n",
        "        self.gru2 = nn.GRU(hidden_size1, hidden_size2, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_size2, output_size)\n",
        "        self.relu = nn.ReLU()\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)\n",
        "        x, _ = self.gru1(x)\n",
        "        x, _ = self.gru2(x)\n",
        "        x = self.linear(x[:, -1, :])\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SdXBl_gHhOTU",
        "outputId": "877a40e8-2766-4e93-c74b-2850a76e3b72"
      },
      "outputs": [],
      "source": [
        "# 모델을 이용하여 학습을 위한 기능 제작\n",
        "\n",
        "def train_model(model, optimizer, criterion, train_loader, val_loader, epochs, device, patience=10):\n",
        "    best_loss = 100\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for i, data in enumerate(val_loader, 0):\n",
        "                inputs, labels = data[0].to(device), data[1].to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "        val_loss /= len(val_loader)\n",
        "        if val_loss < best_loss:\n",
        "            best_loss = val_loss\n",
        "            best_model = model.state_dict()\n",
        "            patience_count = 0\n",
        "        else:\n",
        "            patience_count += 1\n",
        "            if patience_count > patience:\n",
        "                model.load_state_dict(best_model)\n",
        "                print('Early stopping')\n",
        "                break\n",
        "        print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}, Val Loss: {val_loss}')\n",
        "    return model\n",
        "\n",
        "# 모델과 학습기를 이용한 학습\n",
        "modelBI= BILSTM(input_size, hidden_size1, hidden_size2, output_size)\n",
        "optimizer = optim.Adam(modelBI.parameters(), lr=learning_rate)\n",
        "criterion = nn.MSELoss()\n",
        "modelBI = train_model(modelBI, optimizer, criterion, train_loader, val_loader, epochs, device)\n",
        "\n",
        "modelLS= LSTM(input_size, hidden_size1, hidden_size2, output_size)\n",
        "optimizer = optim.Adam(modelLS.parameters(), lr=learning_rate)\n",
        "criterion = nn.MSELoss()\n",
        "modelLS = train_model(modelLS, optimizer, criterion, train_loader, val_loader, epochs, device)\n",
        "\n",
        "modelGR= GRU(input_size, hidden_size1, hidden_size2, output_size)\n",
        "optimizer = optim.Adam(modelGR.parameters(), lr=learning_rate)\n",
        "criterion = nn.MSELoss()\n",
        "modelGR = train_model(modelGR, optimizer, criterion, train_loader, val_loader, epochs, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-u_haF1hOTU"
      },
      "outputs": [],
      "source": [
        "# Tensor를 일반 데이터로 변환 및 역정규화를 통해 실제값으로 변환\n",
        "modelBI.eval()\n",
        "modelLS.eval()\n",
        "modelGR.eval()\n",
        "test_results_BI = []\n",
        "test_results_LS = []\n",
        "test_results_GR = []\n",
        "for x, y in val_loader:\n",
        "    x = x.to(device)\n",
        "    test_results_BI.append(modelBI(x))\n",
        "    test_results_LS.append(modelLS(x))\n",
        "    test_results_GR.append(modelGR(x))\n",
        "test_results_BI = torch.cat(test_results_BI)\n",
        "test_results_LS = torch.cat(test_results_LS)\n",
        "test_results_GR = torch.cat(test_results_GR)\n",
        "test_results_BI = y_scaler.inverse_transform(test_results_BI.cpu().detach().numpy())\n",
        "test_results_LS = y_scaler.inverse_transform(test_results_LS.cpu().detach().numpy())\n",
        "test_results_GR = y_scaler.inverse_transform(test_results_GR.cpu().detach().numpy())\n",
        "x_train_tensor = torch.FloatTensor(scaler.transform(x_train)).to(device)\n",
        "train_results_BI = modelBI(x_train_tensor)\n",
        "train_results_LS = modelLS(x_train_tensor)\n",
        "train_results_GR = modelGR(x_train_tensor)\n",
        "train_results_BI = y_scaler.inverse_transform(train_results_BI.cpu().detach().numpy())\n",
        "train_results_LS = y_scaler.inverse_transform(train_results_LS.cpu().detach().numpy())\n",
        "train_results_GR = y_scaler.inverse_transform(train_results_GR.cpu().detach().numpy())\n",
        "\n",
        "# 학습 및 테스트 데이터의 예측값을 하나의 데이터로 합침\n",
        "results_BI = np.concatenate((train_results_BI.flatten(), test_results_BI.flatten()), axis=0)\n",
        "results_LS = np.concatenate((train_results_LS.flatten(), test_results_LS.flatten()), axis=0)\n",
        "results_GR = np.concatenate((train_results_GR.flatten(), test_results_GR.flatten()), axis=0)\n",
        "\n",
        "# 결과를 데이터프레임(엑셀시트)으로 변환\n",
        "datares = pd.DataFrame()\n",
        "datares[target_column] = dataset[target_column]\n",
        "datares['Inflow_BI-LSTM'] = results_BI\n",
        "datares['Inflow_LSTM'] = results_LS\n",
        "datares['Inflow_GRU'] = results_GR\n",
        "datares = datares.applymap(lambda x: x if x > 0 else 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "UW8nKoYjhOTV",
        "outputId": "947fc61c-c371-49a9-9aaf-e95ae2607e40"
      },
      "outputs": [],
      "source": [
        "# 결과를 엑셀형식으로 확인\n",
        "datares"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5. 결과 검증"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        },
        "id": "Feu6pEr0hOTV",
        "outputId": "b2a827e9-4dee-49dc-d62b-14b89a6d6348"
      },
      "outputs": [],
      "source": [
        "# 수문학 모델의 성능을 평가하기 위한 함수들\n",
        "\n",
        "def calc_MAE(y_true, y_pred):\n",
        "    return np.mean(np.abs(y_true - y_pred))\n",
        "\n",
        "def calc_RMSE(y_true, y_pred):\n",
        "    return np.sqrt(np.mean((y_true - y_pred)**2))\n",
        "\n",
        "def calc_MAPE(y_true, y_pred):\n",
        "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "\n",
        "def calc_NSE(y_true, y_pred):\n",
        "    return 1 - np.sum((y_true - y_pred)**2) / np.sum((y_true - np.mean(y_true))**2)\n",
        "\n",
        "def calc_r2(y_true, y_pred):\n",
        "    mean_t = np.mean(y_true)\n",
        "    mean_p = np.mean(y_pred)\n",
        "    return 1 - (np.sum((y_true - mean_t) * (y_pred - mean_p)) / np.sqrt(np.sum((y_true - mean_t)**2) * np.sum((y_pred - mean_p)**2)))**2\n",
        "\n",
        "def calc_pe(y_true, y_pred):\n",
        "    return ((y_true - y_pred) / y_true) * 100\n",
        "\n",
        "def calc_pbias(y_true, y_pred):\n",
        "    return 100 * np.sum(y_true - y_pred) / np.sum(y_true)\n",
        "\n",
        "def calc_rsr(y_true, y_pred):\n",
        "    return np.sqrt(np.sum((y_true - y_pred)**2) / np.sum((y_true - np.mean(y_true))**2))\n",
        "\n",
        "def calc_mNSE(y_true, y_pred):\n",
        "    return 1 - np.sum((y_true - y_pred)**2) / np.sum((y_true - np.mean(y_true))**2)\n",
        "\n",
        "def calc_md(y_true, y_pred):\n",
        "    return np.sum(np.abs(y_true - y_pred)) / np.sum(np.abs(y_true))\n",
        "\n",
        "def calc_rd(y_true, y_pred):\n",
        "    return np.sum(np.abs(y_true - y_pred)) / np.sum(np.abs(y_pred))\n",
        "\n",
        "def calc_cp(y_true, y_pred):\n",
        "    return 1 - (np.sum((y_true - y_pred)**2) / np.sum((np.abs(y_true - np.mean(y_true)) + np.abs(y_pred - np.mean(y_pred)))**2))\n",
        "\n",
        "def calc_r(y_true, y_pred):\n",
        "    return np.sum(y_true * y_pred) / np.sqrt(np.sum(y_true**2) * np.sum(y_pred**2))\n",
        "\n",
        "def calc_bR2(y_true, y_pred):\n",
        "    return 1 - (np.sum((y_true - y_pred)**2) / np.sum((y_true - np.mean(y_true))**2))\n",
        "\n",
        "def calc_rNSE(y_true, y_pred):\n",
        "    return 1 - (np.sum((y_true - y_pred)**2) / np.sum((y_true - np.mean(y_true))**2))\n",
        "\n",
        "def calc_kge(y_true, y_pred):\n",
        "  r = np.corrcoef(y_true, y_pred)[0, 1]\n",
        "  return 1 - np.sqrt((r - 1)**2 + (np.std(y_pred) / np.std(y_true) - 1)**2 + (np.mean(y_pred) / np.mean(y_true) - 1)**2)\n",
        "\n",
        "def calc_ve(y_true, y_pred):\n",
        "    return np.sum((y_true - y_pred)**2) / np.sum((y_true - np.mean(y_true))**2)\n",
        "\n",
        "def calc_all(y_true, y_pred):\n",
        "    res_dict = {}\n",
        "    res_dict['MAE'] = calc_MAE(y_true, y_pred)\n",
        "    res_dict['RMSE'] = calc_RMSE(y_true, y_pred)\n",
        "    res_dict['NSE'] = calc_NSE(y_true, y_pred)\n",
        "    #res_dict['r2'] = calc_r2(y_true, y_pred)\n",
        "    #res_dict['pbias'] = calc_pbias(y_true, y_pred)\n",
        "    #res_dict['rsr'] = calc_rsr(y_true, y_pred)\n",
        "    #res_dict['mNSE'] = calc_mNSE(y_true, y_pred)\n",
        "    #res_dict['md'] = calc_md(y_true, y_pred)\n",
        "    #res_dict['rd'] = calc_rd(y_true, y_pred)\n",
        "    #res_dict['cp'] = calc_cp(y_true, y_pred)\n",
        "    #res_dict['r'] = calc_r(y_true, y_pred)\n",
        "    #res_dict['bR2'] = calc_bR2(y_true, y_pred)\n",
        "    #res_dict['rNSE'] = calc_rNSE(y_true, y_pred)\n",
        "    #res_dict['kge'] = calc_kge(y_true, y_pred)\n",
        "    #res_dict['ve'] = calc_ve(y_true, y_pred)\n",
        "    return res_dict\n",
        "\n",
        "result_with_metric = {}\n",
        "result_with_metric['Inflow_BI-LSTM'] = calc_all(datares['Inflow'], datares['Inflow_BI-LSTM'])\n",
        "result_with_metric['Inflow_LSTM'] = calc_all(datares['Inflow'], datares['Inflow_LSTM'])\n",
        "result_with_metric['Inflow_GRU'] = calc_all(datares['Inflow'], datares['Inflow_GRU'])\n",
        "df_results = pd.DataFrame(result_with_metric)\n",
        "# 결과를 엑셀형식으로 확인. 평가항목은 MAE, RMSE, NSE\n",
        "df_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "WDBUA4FHhOTW",
        "outputId": "5baaa8ba-1002-4932-e12d-463e7ed916b6"
      },
      "outputs": [],
      "source": [
        "# 결과를 그래프로 시각화\n",
        "import plotly.graph_objects as go\n",
        "fig=go.Figure()\n",
        "#add pred data\n",
        "fig.add_trace(go.Scatter(x=datares.index, y=datares['Inflow_BI-LSTM'], mode='lines', name='Inflow_BI-LSTM', line=dict(color='blue')))\n",
        "fig.add_trace(go.Scatter(x=datares.index, y=datares['Inflow_LSTM'], mode='lines', name='Inflow_LSTM', line=dict(color='red')))\n",
        "fig.add_trace(go.Scatter(x=datares.index, y=datares['Inflow_GRU'], mode='lines', name='Inflow_GRU', line=dict(color='green')))\n",
        "#add real data\n",
        "fig.add_trace(go.Scatter(x=datares.index, y=datares['Inflow'], mode='lines', name='Inflow', line=dict(color='black')))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
